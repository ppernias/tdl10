% ============================================================================
% II. BACKGROUND AND RELATED WORK
% ============================================================================
\section{Background and Related Work}

This section reviews two decades of research on Intelligent Tutoring Systems (2005--2025), focusing on architecture, effectiveness evidence, authoring tools, and the transition to LLMs. We also examine lessons from IMS Learning Design, recent prompt languages, and our prior work with ADL~1.0.

\subsection{Classical ITS Architecture}

The canonical architecture of an Intelligent Tutoring System, established by Nwana \cite{nwana1990} and refined in subsequent work \cite{woolf2009}, comprises four interrelated components:

\begin{enumerate}
    \item \textbf{Domain Model}: Represents the expert knowledge that the system teaches.

    \item \textbf{Student Model}: Maintains a representation of the learner's cognitive state: what they know, what misconceptions they hold, and how they progress.

    \item \textbf{Pedagogical Model}: Contains instructional strategies and decides what action to take given the student's state and the domain.

    \item \textbf{User Interface}: Manages communication between the system and the student.
\end{enumerate}

The \textit{student model} deserves special attention because it is the component that differentiates an ITS from conventional computer-assisted instruction. Techniques such as \textit{model tracing} \cite{anderson1995} compare student actions against a cognitive model, identifying errors in real time. \textit{Knowledge tracing} \cite{corbett1994} probabilistically estimates mastery of specific skills.

\subsection{Effectiveness Evidence}

The effectiveness of ITS is well documented through multiple meta-analyses:

\textbf{VanLehn (2011)} \cite{vanlehn2011} compared 54 studies and found: human tutoring $d = 0.79$; \textit{step-based} ITS $d = 0.76$; \textit{substep-based} ITS $d = 0.40$; \textit{answer-based} CAI $d = 0.31$. The direct comparison of ITS vs. human tutoring yielded $g = -0.11$ (not significant).

\textbf{Kulik and Fletcher (2016)} \cite{kulik2016} analyzed 50 controlled evaluations: median effect size $d = 0.66$; 92\% of evaluations showed superiority over conventional instruction.

\textbf{Ma et al. (2014)} \cite{ma2014} identified \textbf{real-time cognitive diagnosis} and \textbf{adaptive remediation} as the most critical elements for ITS effectiveness.

A crucial finding from VanLehn \cite{vanlehn2011} was that \textit{step-based} tutors (which verify comprehension at each step) are significantly more effective than \textit{answer-based} tutors (which only evaluate final answers). This result informs the design of TDL's Bloom 8-Step Interactive model.

\subsection{Authoring Tools}

Historically, ITS development has required between 200--300 hours of development per hour of instruction \cite{murray1999}. Authoring tools seek to reduce this barrier.

\textbf{CTAT} \cite{aleven2009} introduced \textit{Example-Tracing Tutors}, which can be built ``entirely without programming'' through programming by demonstration. Authors demonstrate desired behaviors, and the system generalizes the rules.

\textbf{GIFT} \cite{sottilare2012} implements a modular architecture with explicit separation between pedagogical module and domain module. This separation allows reusing instructional strategies across different knowledge domains.

\textbf{AutoTutor} \cite{graesser2004} pioneered the use of natural language dialogue for tutoring, with reported effect sizes of 0.4 to 1.5. Its architecture includes a Curriculum Script that organizes questions and a Dialog Advancer that manages the conversation.

However, even these tools require significant technical knowledge and specific software ecosystems that limit their adoption outside research environments.

\subsection{The Transition to LLMs}

Research documents systematic problems with LLMs as tutors. Tack and Piech \cite{tack2022} found that ``current LLMs are not good tutors by default.'' Borchers et al. \cite{borchers2025} confirmed that GPT-4 ``provides overly direct feedback.''

However, early evidence is promising: Pardos and Bhandari \cite{pardos2024} found that hints generated by ChatGPT produced 17\% learning gain vs. 11.62\% for human tutor hints---no significant difference. Kestin et al. \cite{kestin2025} reported that students with an AI tutor outperformed those receiving in-person active instruction.

The key insight is that LLMs require explicit pedagogical structuring to function effectively as tutors. Without such structuring, they default to helpful but pedagogically suboptimal behaviors like providing direct answers.

\subsection{Lessons from IMS Learning Design}

Educational Modeling Language (EML), developed by Rob Koper at Open University of the Netherlands, was the basis for IMS Learning Design (IMS LD v1.0, February 2003) \cite{koper2005}. Van Es and Koper demonstrated that 16 lesson plans from diverse pedagogical traditions could be successfully encoded in IMS LD.

However, Derntl et al. \cite{derntl2012} reported: ``IMS LD has been available since 2003, and yet it has not been widely adopted.'' The identified causes were:

\begin{itemize}
    \item \textbf{Immature ecosystem}: Griffiths et al. \cite{griffiths2005} found that ``round-tripping between tools is not possible.''
    \item \textbf{High effort}: Berggren et al. \cite{berggren2005} documented a 3:1 ratio between preparation and use.
    \item \textbf{Terminology mismatch}: Neumann and Oberhuemer \cite{neumann2008} identified that ``the concepts of the language differ from those a teacher uses for planning.''
\end{itemize}

Crucially, Derntl et al. \cite{derntl2012} found that after 45 minutes of introduction, 78\% of professors achieved conformity with expert solutions. ``The conceptual structure of IMS LD \textbf{does not} impede its use for authoring.'' The barriers were ecosystem-related, not conceptual.

These lessons inform TDL design: (1) conceptual simplicity does not guarantee adoption---ecosystem matters; (2) effort must be proportional to perceived benefit; (3) terminology must align with educators' language; (4) portability reduces ecosystem dependency.

\subsection{Prompt Languages: PDL and POML}

The field of prompt engineering has recently produced formal languages for structuring LLM interactions.

IBM Research developed the \textbf{Prompt Declaration Language (PDL)} \cite{ibm2024pdl}, a YAML-based DSL with formal grammar and type system. PDL enables declarative composition of prompts with control flow, variable binding, and function calls.

Microsoft Research proposed \textbf{POML (Prompt Orchestration Markup Language)} \cite{zhang2025poml}, an HTML-inspired markup language with semantic components such as \texttt{<role>} and \texttt{<task>}. POML incorporates advanced features including a CSS-like style system for separating content from presentation, native multimodal data handling, and a template engine with variables, loops, and conditionals.

However, neither PDL nor POML incorporates instructional design concepts: they lack notions such as learning events, pedagogical sequences, or explicit separation between methodology and educational content. They are designed for developers, not educators.

These languages establish a relevant precedent: it is legitimate and useful to create domain-specific languages for structuring LLM interactions. The question is how to do so for the educational domain.

\subsection{ADL 1.0: The Starting Point}

Our prior work with the Assistant Description Language (ADL 1.0) \cite{pernias2025adl} introduced a structured YAML-based language for encoding educational assistants based on LLMs. ADL 1.0 demonstrated that it is possible to capture a teacher's pedagogical expertise in a formal specification that an LLM can faithfully execute.

ADL 1.0 defined four types of pedagogical tools:

\begin{itemize}
    \item \textbf{Commands} (/command): Self-contained pedagogical actions, from simple explanations to multi-step procedures.
    \item \textbf{Options} (/option): Modifiers that adjust global behavior without associating to specific content.
    \item \textbf{Decorators} (+++decorator): Pedagogical styles that modify how a command is executed (e.g., +++socratic transforms any command into guided questions).
    \item \textbf{Workflows} (/workflow): Automated sequences of commands for complete lessons.
\end{itemize}

A pilot with 30 students in a Tourism course showed positive results: 70\% used the assistant frequently, 83\% rated responses as useful, and 100\% recommended continued use. Teachers perceived the tool as an extension of their pedagogical practice, not a replacement.

However, as ADL 1.0 was applied to structured educational tutoring, limitations emerged that motivated the development of TDL. These limitations are analyzed in detail in Section~III.
